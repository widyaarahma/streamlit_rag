{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac563b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cf21398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (2.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c1b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.51.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a778f2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "Collecting huggingface-hub>=0.20.0\n",
      "  Using cached huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting Pillow\n",
      "  Using cached pillow-11.3.0-cp39-cp39-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.12.2)\n",
      "Collecting transformers<5.0.0,>=4.41.0\n",
      "  Using cached transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "Collecting torch>=1.11.0\n",
      "  Using cached torch-2.7.1-cp39-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp39-cp39-macosx_12_0_arm64.whl (11.1 MB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl (172 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "\u001b[K     |████████████████████████████████| 418 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.2-cp39-cp39-macosx_10_9_universal2.whl (201 kB)\n",
      "\u001b[K     |████████████████████████████████| 201 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "\u001b[K     |████████████████████████████████| 162 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "\u001b[K     |████████████████████████████████| 307 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: urllib3, idna, charset-normalizer, certifi, tqdm, requests, pyyaml, hf-xet, fsspec, filelock, mpmath, MarkupSafe, huggingface-hub, tokenizers, threadpoolctl, sympy, scipy, safetensors, regex, networkx, joblib, jinja2, transformers, torch, scikit-learn, Pillow, sentence-transformers\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.3.0 certifi-2025.7.14 charset-normalizer-3.4.2 filelock-3.18.0 fsspec-2025.7.0 hf-xet-1.1.5 huggingface-hub-0.33.4 idna-3.10 jinja2-3.1.6 joblib-1.5.1 mpmath-1.3.0 networkx-3.2.1 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.4 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.13.1 sentence-transformers-5.0.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.2 torch-2.7.1 tqdm-4.67.1 transformers-4.53.2 urllib3-2.5.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca084278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit==1.35.0 in /opt/anaconda3/lib/python3.12/site-packages (1.35.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (8.1.7)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=16.8 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (23.2)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (2.2.2)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (10.3.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (14.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (2.32.2)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (13.3.5)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (4.11.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (3.1.37)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.35.0) (6.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit==1.35.0) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit==1.35.0) (4.19.2)\n",
      "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit==1.35.0) (0.12.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.35.0) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.35.0) (4.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.3.0->streamlit==1.35.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.3.0->streamlit==1.35.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.3.0->streamlit==1.35.0) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit==1.35.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit==1.35.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit==1.35.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit==1.35.0) (2024.6.2)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich<14,>=10.14.0->streamlit==1.35.0) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich<14,>=10.14.0->streamlit==1.35.0) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<14,>=10.14.0->streamlit==1.35.0) (0.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->altair<6,>=4.0->streamlit==1.35.0) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.35.0) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.35.0) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.35.0) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.35.0) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit==1.35.0) (1.16.0)\n",
      "Requirement already satisfied: pandas==2.2.2 in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas==2.2.2) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas==2.2.2) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas==2.2.2) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.16.0)\n",
      "Requirement already satisfied: sentence-transformers==2.6.1 in /opt/anaconda3/lib/python3.12/site-packages (2.6.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.6.1) (4.51.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.6.1) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.6.1) (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.6.1) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.6.1) (1.4.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.6.1) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.6.1) (0.30.1)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers==2.6.1) (10.3.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.6.1) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.6.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.6.1) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.6.1) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.6.1) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.6.1) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.6.1) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.6.1) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.6.1) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers==2.6.1) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.32.0->sentence-transformers==2.6.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.32.0->sentence-transformers==2.6.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.32.0->sentence-transformers==2.6.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.32.0->sentence-transformers==2.6.1) (2024.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers==2.6.1) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers==2.6.1) (2.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers==2.6.1) (1.3.0)\n",
      "Collecting faiss-cpu==1.7.4\n",
      "  Using cached faiss-cpu-1.7.4.tar.gz (57 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: faiss-cpu\n",
      "  Building wheel for faiss-cpu (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for faiss-cpu \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[21 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /private/var/folders/vw/z88nn85s59nbjgrz304n0h7h0000gp/T/pip-build-env-gruvjv_v/overlay/lib/python3.12/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         License :: OSI Approved :: MIT License\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   self._finalize_license_expression()\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'faiss._swigfaiss' extension\n",
      "  \u001b[31m   \u001b[0m swigging faiss/faiss/python/swigfaiss.i to faiss/faiss/python/swigfaiss_wrap.cpp\n",
      "  \u001b[31m   \u001b[0m swig -python -c++ -Doverride= -I/usr/local/include -Ifaiss -doxygen -module swigfaiss -o faiss/faiss/python/swigfaiss_wrap.cpp faiss/faiss/python/swigfaiss.i\n",
      "  \u001b[31m   \u001b[0m error: command 'swig' failed: No such file or directory\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for faiss-cpu\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build faiss-cpu\n",
      "\u001b[31mERROR: Failed to build installable wheels for some pyproject.toml based projects (faiss-cpu)\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting numpy==1.24.4\n",
      "  Using cached numpy-1.24.4.tar.gz (10.9 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[33 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/anaconda3/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/anaconda3/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/anaconda3/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 137, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     backend = _build_backend()\n",
      "  \u001b[31m   \u001b[0m               ^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/anaconda3/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 70, in _build_backend\n",
      "  \u001b[31m   \u001b[0m     obj = import_module(mod_path)\n",
      "  \u001b[31m   \u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/anaconda3/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "  \u001b[31m   \u001b[0m     return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/vw/z88nn85s59nbjgrz304n0h7h0000gp/T/pip-build-env-egg9w9ph/overlay/lib/python3.12/site-packages/setuptools/__init__.py\", line 16, in <module>\n",
      "  \u001b[31m   \u001b[0m     import setuptools.version\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/vw/z88nn85s59nbjgrz304n0h7h0000gp/T/pip-build-env-egg9w9ph/overlay/lib/python3.12/site-packages/setuptools/version.py\", line 1, in <module>\n",
      "  \u001b[31m   \u001b[0m     import pkg_resources\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/vw/z88nn85s59nbjgrz304n0h7h0000gp/T/pip-build-env-egg9w9ph/overlay/lib/python3.12/site-packages/pkg_resources/__init__.py\", line 2172, in <module>\n",
      "  \u001b[31m   \u001b[0m     register_finder(pkgutil.ImpImporter, find_on_path)\n",
      "  \u001b[31m   \u001b[0m                     ^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Requirement already satisfied: openai==0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/anaconda3/lib/python3.12/site-packages (from openai==0.28.0) (2.32.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from openai==0.28.0) (4.66.4)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from openai==0.28.0) (3.9.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.20->openai==0.28.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.20->openai==0.28.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.20->openai==0.28.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.20->openai==0.28.0) (2024.6.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai==0.28.0) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai==0.28.0) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai==0.28.0) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai==0.28.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai==0.28.0) (1.9.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install streamlit==1.35.0\n",
    "! pip install pandas==2.2.2\n",
    "! pip install sentence-transformers==2.6.1\n",
    "! pip install faiss-cpu==1.7.4\n",
    "! pip install numpy==1.24.4\n",
    "! pip install openai==0.28.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6224402e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (0.28.0)\n",
      "Collecting openai\n",
      "  Using cached openai-1.97.0-py3-none-any.whl (764 kB)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.0\n",
      "    Uninstalling openai-0.28.0:\n",
      "      Successfully uninstalled openai-0.28.0\n",
      "Successfully installed openai-1.97.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77062a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai==0.28\n",
      "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from openai==0.28) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from openai==0.28) (4.67.1)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.12.14-cp39-cp39-macosx_11_0_arm64.whl (467 kB)\n",
      "\u001b[K     |████████████████████████████████| 467 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from requests>=2.20->openai==0.28) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from requests>=2.20->openai==0.28) (2025.7.14)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from requests>=2.20->openai==0.28) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from requests>=2.20->openai==0.28) (2.5.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.6.3-cp39-cp39-macosx_11_0_arm64.whl (44 kB)\n",
      "\u001b[K     |████████████████████████████████| 44 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.2-cp39-cp39-macosx_11_0_arm64.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.1-cp39-cp39-macosx_11_0_arm64.whl (89 kB)\n",
      "\u001b[K     |████████████████████████████████| 89 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.4.0\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.7.0-cp39-cp39-macosx_11_0_arm64.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2 in /Users/wrahma01/Library/Python/3.9/lib/python/site-packages (from aiosignal>=1.4.0->aiohttp->openai==0.28) (4.12.2)\n",
      "Installing collected packages: propcache, multidict, frozenlist, yarl, attrs, async-timeout, aiosignal, aiohappyeyeballs, aiohttp, openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.97.0\n",
      "    Uninstalling openai-1.97.0:\n",
      "      Successfully uninstalled openai-1.97.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.14 aiosignal-1.4.0 async-timeout-5.0.1 attrs-25.3.0 frozenlist-1.7.0 multidict-6.6.3 openai-0.28.0 propcache-0.3.2 yarl-1.20.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai==0.28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8553f493",
   "metadata": {},
   "source": [
    "# Large Language Model & Retrieval Augmented Generation\n",
    "\n",
    "- Apa itu LLM (Large Language Model)?\n",
    "\n",
    "LLM adalah model kecerdasan buatan yang dilatih dengan banyak teks untuk bisa mengerti, menulis, dan menjawab pertanyaan dalam bahasa manusia. LLM ini bisa menjawab pertanyaan, bantu menulis, merangkum teks, dan banyak hal lainnya.\n",
    "\n",
    "- Apa itu RAG (Retrieval-Augmented Generation)?\n",
    "\n",
    "RAG adalah cara untuk menggabungkan kekuatan LLM + data kamu sendiri. Atau sederhananya  **RAG = cari data (Retrieval) + jawab pakai LLM (Generation)**\n",
    "\n",
    "- Kapan butuh RAG?\n",
    "    - Kalau LLM tidak tahu data kamu (misalnya data internal perusahaan)\n",
    "    - Kalau kamu ingin jawaban berbasis bukti/data, bukan cuma jawaban umum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669db7b8",
   "metadata": {},
   "source": [
    "## Alur RAG\n",
    "\n",
    "Dalam melakukan RAG, terdapat beberapa tahapan yang perlu dilakukan dari data tabular yang dimiliki agar nantinya model LLM dapat memberikan jawaban yang sesuai. Berikut beberapa tahapannya. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef01373",
   "metadata": {},
   "source": [
    "### Step 1: Mempersiapkan Data Tabular\n",
    "\n",
    "Agar nantinya LLM dapat menghasilkan jawaban yang relevan berdasarkan data, kita perlu mempersiapkan data dalam bentukan tabular terlebih dahulu, dimana bentukan data tabular yang dimaksud di sini adalah data yang berisikan kolom-kolom data dan juga baris.\n",
    "\n",
    "Bentukan format data yang digunakan bisa bervariasi seperti excel, csv, atau lainnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd9fede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>nama_produk</th>\n",
       "      <th>kategori</th>\n",
       "      <th>harga</th>\n",
       "      <th>stok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Laptop X</td>\n",
       "      <td>Elektronik</td>\n",
       "      <td>12000000</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Headphone Y</td>\n",
       "      <td>Elektronik</td>\n",
       "      <td>1500000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Smartphone Z</td>\n",
       "      <td>Elektronik</td>\n",
       "      <td>8000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Monitor A</td>\n",
       "      <td>Elektronik</td>\n",
       "      <td>3000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Keyboard B</td>\n",
       "      <td>Aksesoris</td>\n",
       "      <td>500000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   nama_produk    kategori     harga  stok\n",
       "0   1      Laptop X  Elektronik  12000000    15\n",
       "1   2   Headphone Y  Elektronik   1500000     5\n",
       "2   3  Smartphone Z  Elektronik   8000000     3\n",
       "3   4     Monitor A  Elektronik   3000000     8\n",
       "4   5    Keyboard B   Aksesoris    500000    25"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "product = pd.read_csv(\"/Users/wrahma01/Documents/Building LLM Applications for Structured Data Insights/data/products.csv\")\n",
    "product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34b1650",
   "metadata": {},
   "source": [
    "### Step 2: Transformasi Data ke Format Teks\n",
    "\n",
    "Bentukan data tabular merupakan bentukan yang suliit dipelajari oleh LLM dikarenakan setiap kolomnya berdiri secara individual, agar nantinya lebih mudah untuk dipelajari, tahapan yang perlu dilakukan adalah mengubah 1 baris data yang terpisah antara setiap kolom, menjadi 1 kalimat penuh.\n",
    "\n",
    "Berikut adalah contohnya:\n",
    "\n",
    "**Data Tabular**\n",
    "| Nama | Gaji | Divisi  |\n",
    "| ---- | ---- | ------- |\n",
    "| Rina | 10jt | Finance |\n",
    "\n",
    "**Hasil Transformasi**\n",
    "```python\n",
    "Nama: Rina | Gaji: 10jt | Divisi: Finance\n",
    "```\n",
    "\n",
    "Dengan bentuk ini, LLM bisa \"membaca\" dan \"menalar\" seperti membaca kalimat biasa. Selain dari itu, LLM jadi tahu bahwa “Rina” berhubungan dengan “Finance” dan “10jt” — hal yang tidak otomatis dipahami kalau datanya hanya angka-angka di tabel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f19d94f",
   "metadata": {},
   "source": [
    "Terdapat sebuah fungsi `.agg()` yang biasanya digunakan untuk melakukan perhitungan, fungsi tersebut dapat kita manfaatkan lebih lanjut untuk menggabungkan informasi dari beberapa kolom. \n",
    "\n",
    "Fungsi tersebut nantinya akan digabungkan dengan metode `.join()`, metode `.join()` digunakan untuk menggabungkan string, bukan untuk menggabungkan DataFrame seperti merge(). \n",
    "\n",
    "Agar nantinya semua fungsi tersebut bisa digabungkan, satu syarat yang harus dipenuhi adalah semua kolom harus memiliki tipe data yang sama. Hal tersebut bisa dipenuhi dengan menggunakan fungsi `.astype()`.\n",
    "\n",
    "```python\n",
    "df[['kolom_1', 'kolom_2']].astype('tipe_data').agg(' | '.join, axis = 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db9af997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>nama_produk</th>\n",
       "      <th>kategori</th>\n",
       "      <th>harga</th>\n",
       "      <th>stok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Laptop X</td>\n",
       "      <td>Elektronik</td>\n",
       "      <td>12000000</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id nama_produk    kategori     harga  stok\n",
       "0   1    Laptop X  Elektronik  12000000    15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec9e080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Laptop X | 12000000\n",
       "1     Headphone Y | 1500000\n",
       "2    Smartphone Z | 8000000\n",
       "3       Monitor A | 3000000\n",
       "4       Keyboard B | 500000\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mari gabungkan 2 buah kolom terlebih dahulu \n",
    "product[['nama_produk', 'harga']].astype('str').agg(' | '.join, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3581c",
   "metadata": {},
   "source": [
    "Setelah berhasil memahami bagaimana cara kerja menggabungkan beberapa informasi data menjadi satu kesatuan, tahapan berikutnya adalah kita harus mempersiapkan fungsi untuk kedepannya bisa menerima semua inputan data ataupun kolom yang ingin diproses.\n",
    "\n",
    "Hal ini diperlukan karena tidak tentu semua kolom harus diproses atau diambil untuk dipahami.\n",
    "\n",
    "Berikut kerangka berfikir dalam pembuatan fungsinya:\n",
    "- Siapan sebuah fungsi yang bisa menampung 2 parameter\n",
    "    + Parameter untuk menampung variabel dataframe\n",
    "    + Parameter untuk memilih kolom-kolom yang diperlukan\n",
    "- Memanfaatkan fungsi yang sudah dibuat tadi, tapi ada hal yang perlu diganti, yaitu menyesuapkan nama variabel dataframe dan nama kolom, sesuai dengan parameter yang dipersiapkan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ee1c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat sebuah fungsi\n",
    "def penggabungan_kolom(df, kolom_data):\n",
    "    df['teks'] = df[kolom_data].astype('str').agg(' | '.join, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e0e6ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>nama_produk</th>\n",
       "      <th>kategori</th>\n",
       "      <th>harga</th>\n",
       "      <th>stok</th>\n",
       "      <th>teks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Laptop X</td>\n",
       "      <td>Elektronik</td>\n",
       "      <td>12000000</td>\n",
       "      <td>15</td>\n",
       "      <td>1 | Laptop X | Elektronik | 12000000 | 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Headphone Y</td>\n",
       "      <td>Elektronik</td>\n",
       "      <td>1500000</td>\n",
       "      <td>5</td>\n",
       "      <td>2 | Headphone Y | Elektronik | 1500000 | 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Smartphone Z</td>\n",
       "      <td>Elektronik</td>\n",
       "      <td>8000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3 | Smartphone Z | Elektronik | 8000000 | 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Monitor A</td>\n",
       "      <td>Elektronik</td>\n",
       "      <td>3000000</td>\n",
       "      <td>8</td>\n",
       "      <td>4 | Monitor A | Elektronik | 3000000 | 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Keyboard B</td>\n",
       "      <td>Aksesoris</td>\n",
       "      <td>500000</td>\n",
       "      <td>25</td>\n",
       "      <td>5 | Keyboard B | Aksesoris | 500000 | 25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   nama_produk    kategori     harga  stok  \\\n",
       "0   1      Laptop X  Elektronik  12000000    15   \n",
       "1   2   Headphone Y  Elektronik   1500000     5   \n",
       "2   3  Smartphone Z  Elektronik   8000000     3   \n",
       "3   4     Monitor A  Elektronik   3000000     8   \n",
       "4   5    Keyboard B   Aksesoris    500000    25   \n",
       "\n",
       "                                          teks  \n",
       "0    1 | Laptop X | Elektronik | 12000000 | 15  \n",
       "1   2 | Headphone Y | Elektronik | 1500000 | 5  \n",
       "2  3 | Smartphone Z | Elektronik | 8000000 | 3  \n",
       "3     4 | Monitor A | Elektronik | 3000000 | 8  \n",
       "4     5 | Keyboard B | Aksesoris | 500000 | 25  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penggabungan_kolom(df = product, kolom_data=['id', 'nama_produk', 'kategori','harga', 'stok'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf7212c",
   "metadata": {},
   "source": [
    "### Step 3: Mencari Kesamaan Teks\n",
    "\n",
    "####  Step 3.1: Embeddng Teks\n",
    "\n",
    "Pada tahapan yang dilakukan adalah mengubah teks menjadi representasi angka (vektor) yang menangkap makna atau arti dari teks tersebut. Makna atau arti yang dimaksud di sini adalah untuk mengetahui kedekatan atau kemiripan antara satu data dengan data lainnya.\n",
    "\n",
    "Informasi mengenai kemiripan antara satu data dengan yang lain penting dikarenakan tujuan awal dari RAG adalah mengambil informasi data yang relevan dengan pertanyaan yang diberikan, untuk menghasilkan jawaban LLM yang relevan. \n",
    "\n",
    "**Model: paraphrase-MiniLM-L6-v2**, model ini adalah bagian dari library Sentence Transformers, yang dibangun di atas BERT dan sangat cocok untuk melakukan embedding teks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c570e7",
   "metadata": {},
   "source": [
    "Dari bantuan di atas, hal yang dapat dilakukan adalah setiap kalimat akan diubah menjadi bentukan vektor. Bentukan vektor tersebut akan dihasilkan akan tergantung dengan semua data train yang dimiliki oleh model **paraphrase-MiniLM-L6-v2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daa05feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wrahma01/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/wrahma01/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# mempersiapkan model embedding yang akan digunakan\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "586b596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contoh kata output yang akan digunakan\n",
    "corpus_query = \"How I learn AI?\"\n",
    "\n",
    "# contoh kata output yang akan digunakan\n",
    "corpus_ouput = [\n",
    "    \"Saya cinta machine learning.\",\n",
    "    \"Python adalah bahasa yang keren.\",\n",
    "    \"Deep learning meningkatkan klasifikasi gambar.\",\n",
    "    \"Artificial intelligence adalah masa depan.\",\n",
    "    \"Daftar di Akademi AI!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309c74c6",
   "metadata": {},
   "source": [
    "Agar nantinya bisa diubah menjadi bentukan vector, fungsi yang akan digunakan adalah `encode()` dan terdapat parameter yang harus diisi `convert_to_numpy=True` tujuannya untuk output dari embedding akan dikembalikan dalam bentuk numpy.ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7929110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mempersiapkan model embedding yang akan digunakan\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "240ee104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.30131885e-01, -4.97135371e-01, -1.91070244e-01, -8.16830933e-01,\n",
       "        4.97040808e-01,  5.74899077e-01,  1.03566611e+00, -2.33822986e-01,\n",
       "       -5.72757013e-02,  1.06510438e-01,  3.91608775e-01,  1.06586829e-01,\n",
       "        1.69848278e-01,  1.16042174e-01, -2.48942941e-01, -2.52282709e-01,\n",
       "       -3.64941627e-01,  4.29967076e-01, -2.83076078e-01, -3.97258341e-01,\n",
       "       -2.01484099e-01, -2.92198211e-01,  6.35252148e-02,  1.02740742e-01,\n",
       "        4.27272797e-01,  7.47818291e-01,  1.51754931e-01, -1.67966023e-01,\n",
       "        5.46775222e-01, -4.32285458e-01,  8.72432411e-01, -4.22608107e-01,\n",
       "        6.00874364e-01, -1.04316071e-01, -1.77313581e-01, -4.81827199e-01,\n",
       "       -5.37118077e-01, -1.13151617e-01, -2.07263306e-01, -1.95097759e-01,\n",
       "       -4.01369750e-01,  3.54159087e-01,  2.07943171e-01, -2.64321864e-01,\n",
       "        8.45733762e-01, -1.09710440e-03,  1.06256127e-01, -3.91945213e-01,\n",
       "        5.07670403e-01,  2.36963525e-01, -5.72868645e-01, -1.15839608e-01,\n",
       "        1.36171564e-01,  1.01161480e-01, -2.25572929e-01,  8.33681405e-01,\n",
       "        4.49290484e-01,  1.26958147e-01, -4.30275947e-01, -3.65047961e-01,\n",
       "       -3.18246692e-01, -3.24071229e-01,  2.22068161e-01, -3.44709083e-02,\n",
       "       -1.34893740e-02,  2.82089680e-01, -1.26759157e-01, -3.02287579e-01,\n",
       "        4.31314334e-02, -3.39939535e-01,  4.78360876e-02,  3.95120792e-02,\n",
       "       -6.22170031e-01, -2.36396282e-03,  2.57479876e-01,  2.58258671e-01,\n",
       "        5.49545102e-02, -5.45700192e-02,  7.25220025e-01,  7.52838135e-01,\n",
       "        1.45425811e-01,  2.99429595e-01,  1.13307782e-01,  4.95804965e-01,\n",
       "        5.61203174e-02,  2.35316381e-01, -6.75655976e-02,  4.18078393e-01,\n",
       "        8.98778066e-02,  5.00618935e-01,  4.16076183e-03,  4.80598398e-02,\n",
       "       -3.51425499e-01, -1.76781341e-01,  1.21134734e+00, -1.84343010e-01,\n",
       "       -2.89413363e-01, -3.23241889e-01, -6.59564212e-02,  8.09550107e-01,\n",
       "       -7.67910838e-01, -5.56910574e-01, -9.77901816e-01, -1.21896282e-01,\n",
       "       -9.94709849e-01, -4.34457004e-01,  8.90013427e-02, -9.25497860e-02,\n",
       "        3.19181770e-01, -3.10069621e-01, -2.90604264e-01, -2.22841635e-01,\n",
       "       -8.92326087e-02,  4.08954732e-02,  4.28624779e-01,  3.69902939e-01,\n",
       "       -2.11643457e-01,  1.58619881e-03,  2.13058129e-01,  7.12149918e-01,\n",
       "        2.52981752e-01, -3.18698972e-01,  1.73111573e-01,  1.26761627e+00,\n",
       "        2.27448389e-01, -2.44172439e-01, -2.40538999e-01, -4.76207256e-01,\n",
       "       -1.66661628e-02,  2.83209652e-01,  1.01524961e+00,  1.15117263e-02,\n",
       "       -2.65149772e-01, -2.34886572e-01,  3.70376050e-01,  7.90029392e-02,\n",
       "       -3.78732756e-02, -6.34628177e-01, -5.20394444e-01,  2.57450193e-01,\n",
       "       -7.04174161e-01,  6.12706244e-01, -2.37371296e-01, -1.54472634e-01,\n",
       "       -1.70954373e-02,  3.67425919e-01,  1.71589524e-01,  4.27975319e-03,\n",
       "        2.20078692e-01, -8.18383634e-01,  2.87716985e-02, -5.04866302e-01,\n",
       "        3.24626774e-01,  4.23959255e-01,  2.50682771e-01, -7.05865100e-02,\n",
       "        9.93471265e-01,  2.70302463e-02, -4.36656654e-01, -1.33239792e-03,\n",
       "       -7.69788206e-01, -3.61035503e-02, -2.74821613e-02, -6.08469427e-01,\n",
       "       -2.24245861e-01,  5.00074983e-01,  1.49845377e-01,  2.66415656e-01,\n",
       "        1.80647492e-01,  1.03724349e+00, -2.67815381e-01, -4.04378682e-01,\n",
       "        8.64135444e-01,  1.79769382e-01,  2.18370363e-01, -7.32354522e-02,\n",
       "       -5.07596076e-01,  3.69880706e-01, -5.01374602e-02,  2.50262082e-01,\n",
       "       -2.12863609e-01, -4.32843268e-01,  8.17532957e-01,  3.69170994e-01,\n",
       "        7.41150901e-02,  5.90861022e-01, -1.24808684e-01,  2.53001511e-01,\n",
       "       -8.34312499e-01,  3.79650109e-02, -4.78836596e-01,  1.02854156e+00,\n",
       "       -2.90916264e-01,  7.49601245e-01, -1.25997111e-01, -5.61399281e-01,\n",
       "        1.87398627e-01,  6.59297183e-02, -4.34405804e-01,  3.43222201e-01,\n",
       "       -2.62783796e-01, -2.26468965e-01, -9.17103440e-02, -5.99554241e-01,\n",
       "       -3.10035288e-01, -8.02286923e-01, -3.86684895e-01,  6.08634949e-02,\n",
       "       -4.20558900e-01,  3.51065367e-01, -4.13642317e-01, -2.77863909e-02,\n",
       "        1.35794774e-01, -4.82389539e-01,  1.92546576e-01, -6.31355226e-01,\n",
       "        2.98722684e-01, -1.93407387e-01, -5.89927256e-01, -2.21707731e-01,\n",
       "       -9.15004373e-01,  1.19276084e-01, -2.67656267e-01, -2.94736717e-02,\n",
       "        1.12619333e-01, -1.17404532e+00, -8.96034896e-01,  2.54200865e-02,\n",
       "       -1.31056041e-01,  4.99418527e-01,  5.72306335e-01,  5.43720067e-01,\n",
       "        4.13124353e-01,  1.93574116e-01,  1.20870933e-01, -1.97495297e-01,\n",
       "       -6.04637206e-01,  1.60463065e-01,  1.52051061e-01, -1.64468184e-01,\n",
       "       -7.31321797e-02,  9.70092490e-02,  1.09054660e-02,  5.03632665e-01,\n",
       "       -7.38690048e-02,  6.54315948e-01,  7.38161281e-02, -2.97626823e-01,\n",
       "       -2.16226771e-01,  6.37660682e-01,  3.79370973e-02,  2.26297110e-01,\n",
       "        5.25658250e-01, -1.52669057e-01, -1.57429591e-01, -3.70083660e-01,\n",
       "        2.93321759e-01, -8.84892233e-03, -3.76276150e-02,  3.89791429e-01,\n",
       "        2.49694183e-01, -6.57514274e-01,  4.72655967e-02,  1.91427320e-01,\n",
       "       -1.76984891e-01, -2.98298270e-01, -8.65597844e-01, -4.12916005e-01,\n",
       "        1.00379579e-01, -7.29948282e-03, -1.44777671e-01,  1.20681608e-02,\n",
       "        5.32018363e-01,  5.06879508e-01, -5.23087420e-02, -9.97976184e-01,\n",
       "        3.45921330e-02, -7.49609292e-01, -2.00756580e-01, -6.15904927e-01,\n",
       "        1.86084528e-02, -3.98498535e-01, -2.31580406e-01,  2.44927764e-01,\n",
       "       -2.19705448e-01, -5.16937852e-01, -1.43565670e-01, -1.55931875e-01,\n",
       "       -1.06614077e+00, -5.04182279e-01, -6.25616252e-01,  9.14002001e-01,\n",
       "       -6.07156277e-01,  1.58130318e-01, -3.19169521e-01,  3.31781864e-01,\n",
       "        1.20219672e+00, -1.36923522e-01, -7.30763018e-01, -8.13928321e-02,\n",
       "       -1.74499542e-01,  2.94975221e-01,  2.33631089e-01, -7.95074642e-01,\n",
       "        6.46220967e-02, -1.79414675e-01,  5.75525343e-01,  3.01410943e-01,\n",
       "        1.45439625e-01,  7.57902563e-01,  5.46385467e-01,  5.08902669e-01,\n",
       "        4.75420713e-01,  2.87622303e-01, -4.19192940e-01,  3.22343856e-01,\n",
       "        2.68367380e-01, -5.31796277e-01, -3.16141784e-01, -1.70161039e-01,\n",
       "       -2.66115367e-01, -2.96756476e-01,  7.10655212e-01,  4.83260959e-01,\n",
       "       -5.34070075e-01,  5.58553934e-01, -7.45341718e-01,  1.77762166e-01,\n",
       "       -1.13880979e-02, -3.50254446e-01,  2.30698660e-01, -2.71707624e-01,\n",
       "        2.09923178e-01,  5.03781140e-01,  2.11362362e-01,  2.05330983e-01,\n",
       "        3.29161972e-01,  6.88302517e-02,  2.87710726e-01, -7.30534196e-01,\n",
       "        4.35148209e-01, -2.22600866e-02, -4.39223439e-01, -5.02087250e-02,\n",
       "       -1.89921930e-01, -4.70623285e-01,  2.14876011e-01, -1.49715051e-01,\n",
       "       -4.45675522e-01,  9.58263040e-01, -4.03979152e-01,  2.53472120e-01,\n",
       "        3.75838608e-01,  1.15621909e-01,  3.05638164e-01,  1.71690732e-01,\n",
       "        6.72796071e-01, -2.43384704e-01, -6.49249256e-02, -2.42854863e-01,\n",
       "       -5.08492649e-01,  1.95287496e-01, -3.04418623e-01, -7.99134672e-01,\n",
       "        1.50482625e-01,  2.56606996e-01,  2.04186305e-01, -9.81567323e-01,\n",
       "        2.99214095e-01,  3.59642133e-02,  1.93559751e-01,  1.24434792e-01,\n",
       "       -3.73342067e-01,  2.44209245e-01,  1.07070720e+00,  4.87237364e-01,\n",
       "        5.29119134e-01, -3.34400713e-01,  8.86342153e-02,  6.28149807e-01,\n",
       "       -1.84001848e-01,  2.93919802e-01, -5.73905818e-02, -1.55911952e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_query = model.encode(corpus_query, convert_to_numpy= True)\n",
    "embedding_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60cc73dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_output = model.encode(corpus_ouput, convert_to_numpy= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20b873",
   "metadata": {},
   "source": [
    "Dari semua kalimat yang berada di atas, nantinya akan coba dicari kesamaanya dengan menghitung *cosine similarity*. \n",
    "\n",
    "- **Semakin mendekati 1**, maka akan dianggap semakin mirip\n",
    "- **Semakin mendekati 0**, maka akan dianggap semakin tidak mirip\n",
    "\n",
    "Fungsi yang digunakan adalah `util.cos_sim()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b067b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3704, 0.0312, 0.2236, 0.4614, 0.4938]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_scores = util.cos_sim(embedding_query, embedding_output)\n",
    "cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37dcf84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How I learn AI?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acc30aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Saya cinta machine learning.',\n",
       " 'Python adalah bahasa yang keren.',\n",
       " 'Deep learning meningkatkan klasifikasi gambar.',\n",
       " 'Artificial intelligence adalah masa depan.',\n",
       " 'Daftar di Akademi AI!']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_ouput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4430be2f",
   "metadata": {},
   "source": [
    "Metode di atas dinamakan **Pencarian Semantik**, karena tujuan dari pencariannya adalah melihat kontekstual satu kalimat, tidak hanya berdasarkan apakah sebuah kata itu ada atau tidak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f61d2",
   "metadata": {},
   "source": [
    "> Bagaimana nantinya kalo kita coba terapkan pada data kita? Data manakan yang paling mendekati, jika diberikan pertanyaan **Laptop untuk bekerja?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "032318cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persiapan query & embedding baru\n",
    "query =  \"laptop untuk bekerja\"\n",
    "embedding_query_baru = model.encode(query, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b29d257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persiapan embedding untuk data lama\n",
    "embedding_dataframe = model.encode(product['teks'], convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "546b8ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4676, 0.3058, 0.2649, 0.3067, 0.2315]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# menghtiung cosine similarity\n",
    "cosine_scores = util.cos_sim(embedding_query_baru, embedding_dataframe)\n",
    "cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "814026eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1 | Laptop X | Elektronik | 12000000 | 15\n",
       "1     2 | Headphone Y | Elektronik | 1500000 | 5\n",
       "2    3 | Smartphone Z | Elektronik | 8000000 | 3\n",
       "3       4 | Monitor A | Elektronik | 3000000 | 8\n",
       "4       5 | Keyboard B | Aksesoris | 500000 | 25\n",
       "Name: teks, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product['teks']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063086f6",
   "metadata": {},
   "source": [
    "### Step 3.2: FAISS Indexing Dari Pertanyaan \n",
    "\n",
    "FAISS (Facebook AI Similarity Search) adalah library dari Facebook AI Research yang dirancang untuk pencarian cepat terhadap vektor berdimensi tinggi. FAISS sangat sering digunakan dalam aplikasi seperti semantic search, image similarity, dan retrieval dalam RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "**Cara Kerja FAISS**\n",
    "\n",
    "1. Mengambil nilai vektor dari hasil embedding\n",
    "2. Melakukan pembuatan index & menambahkan informasi index\n",
    "3. Melakukan pencarian berdasarkan pertanyaan yang diberikan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ddfaa5",
   "metadata": {},
   "source": [
    "Dikarenakan untuk melakukan indexing yang menggabungkan antara cosine similarity dan metode FAIIS tidak bisa secara langsung, melainkan harus dilakukan secara manual. \n",
    "\n",
    "Maka dari itu mari kita coba buat perhitungan cosine similarity secara manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e818c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# melakukan perhitungan\n",
    "embedding_dataframe = embedding_dataframe / np.linalg.norm(embedding_dataframe, axis=1, keepdims=True)\n",
    "embedding_dataframe = embedding_dataframe.astype('float32')\n",
    "\n",
    "# mengambil nilai embedding\n",
    "dimension = embedding_dataframe.shape[1]\n",
    "dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dd8d59",
   "metadata": {},
   "source": [
    "Untuk penambahan informasi index, bisa menggunakan fungsi `IndexFlatL2()` dan hasilnya akan ditambahkan ke hasil embedding dengan menggunakan `.add()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b433639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embedding_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fdda4f",
   "metadata": {},
   "source": [
    "Dari hasil dimensinya jika dibandingkan kembali dengan contoh query pertanyaan di atas, akan seperti di bawah ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a34f1b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'laptop untuk bekerja'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a42eb3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index_query = model.encode([query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "130603b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(embedding_index_query, k = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c0d084a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44.40567 , 46.680927]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68b5c885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3167a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4676, 0.3058, 0.2649, 0.3067, 0.2315]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56e2a93",
   "metadata": {},
   "source": [
    "- `D:` jarak (distance) antara query_vector dengan vektor-vektor terdekat.\n",
    "- `I:` indeks dari vektor-vektor yang paling mirip — ini yang kamu cari."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0665b9b",
   "metadata": {},
   "source": [
    "Mari kita gabungkan perhitungan embeddings sampai dengan proses pembuatan index menjadi sebuaah fungsi.\n",
    "\n",
    "Berikut kerangka berfikir dalam pembuatan fungsinya:\n",
    "\n",
    "- Siapan sebuah fungsi yang bisa menampung teks dari data\n",
    "- Melakukan perhitungan cosine similarity\n",
    "- Melakukan indexing\n",
    "     - Mengambil informasi shape\n",
    "     - Melakukan pembutan index berdasarkan hasil embedding cosine similarity\n",
    "     - Menerapkan hasil index ke embediings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "953a1532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat sebuah fungsi\n",
    "def build_faiss_index_cosine(teks):\n",
    "    # Bagian untuk melakukan embeddings\n",
    "    embedding = model.encode(teks , convert_to_numpy=True)\n",
    "\n",
    "    # Melakukan perhitungan cosine\n",
    "    embedding = embedding / np.linalg.norm(embedding, axis=1, keepdims=True)\n",
    "    embedding = embedding.astype('float32')\n",
    "\n",
    "    # Melakukan indexing\n",
    "    dimension = embedding.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embedding)\n",
    "\n",
    "    return index, embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52988efa",
   "metadata": {},
   "source": [
    "[Optional] Pengambilan sumber informasi berdasarkan berapa banyak informasi yang paling dekat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22ce9cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat sebuah fungsi\n",
    "def retrieve(query, index, df, top_k=3):\n",
    "    # 1. Encode dan normalisasi query\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "    query_embedding = query_embedding.astype(\"float32\")\n",
    "\n",
    "    # 2. Search ke FAISS\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    # 3. Ambil baris data sesuai hasil indexing\n",
    "    result_df = df.iloc[indices[0]].copy()\n",
    "    result_df['similarity_score'] = scores[0]\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac77f84",
   "metadata": {},
   "source": [
    "# Bagaimana LLM Bekerja [Optional]\n",
    "\n",
    "## 1. Bagaimana Model \"Belajar\" dari Pertanyaan?\n",
    "\n",
    "Model LLM tidak belajar dari data setelah dilatih. Tapi ia memahami dan menafsirkan pertanyaan berdasarkan *pretraining* yang sudah dilakukan atas triliunan token.\n",
    "\n",
    "```\n",
    "Bagaimana cara belajar Python untuk pemula?\n",
    "```\n",
    "\n",
    "Model akan:\n",
    "\n",
    "- Memecah pertanyaan menjadi token\n",
    "    + Token adalah unit terkecil dari teks yang digunakan oleh model bahasa seperti GPT saat membaca dan memproses input. Token bisa berupa:\n",
    "        * Kata penuh → seperti \"Python\"\n",
    "        * Sebagian kata → seperti \"Py\", \"thon\"\n",
    "        * Tanda baca → seperti \",\", \".\", atau \"!\"\n",
    "        * Spasi juga bisa dihitung sebagai token\n",
    "- Menggunakan representasi internal yang sangat besar (disebut embedding) untuk memahami struktur kalimat, maksud, dan konteks\n",
    "\n",
    "Contohnya, kata \"belajar Python\" akan dikenali sebagai sesuatu yang mengarah ke programming, tutorial, atau konsep dasar coding.\n",
    "\n",
    "## 2. Mencocokkan ke Data yang Paling Dekat\n",
    "\n",
    "```\n",
    "[User Query] → \"Apa manfaat dari vitamin C?\"\n",
    "         ↓\n",
    "[Query Embedding]\n",
    "         ↓\n",
    "[Search via FAISS / Vector DB]\n",
    "         ↓\n",
    "[Temukan dokumen yang mirip secara makna]\n",
    "         ↓\n",
    "[Gabungkan query + dokumen → kirim ke GPT-4.1-mini]\n",
    "         ↓\n",
    "[Jawaban: \"Vitamin C penting untuk sistem imun dan antioksidan.\"]\n",
    "```\n",
    "\n",
    "## 3. Bagaimana LLM Menjawab?\n",
    "\n",
    "Setelah dokumen hasil pencarian diberikan (biasanya sebagai context), model akan:\n",
    "\n",
    "- Memproses seluruh input (pertanyaan + dokumen)\n",
    "- Menggunakan kemampuannya untuk:\n",
    "    - Menemukan bagian yang paling relevan\n",
    "    - Menyusun kalimat berdasarkan jawaban yang konsisten secara logika dan bahasa\n",
    "- Tidak mengambil langsung dari dokumen, tapi merangkai ulang dengan pemahaman model\n",
    "\n",
    "### 3.1 Proses Merangkai Kalimat\n",
    "\n",
    "Proses merangkai kata-kata jawaban akan berdasarkan perhitungan jarak dan hasil probabilitas yang paling sesuai dari hasil data-data latih yang model miliki, ditambahkan dengan data-data baru yang diinput.\n",
    "\n",
    "- Untuk perhitungan jarak, konsep yang akan digunakan akan sama dengan konsep indexing dengan metode FAISS\n",
    "- Perhitungan probabilitas akan memanfaatkan metode klasifikasi dalam menentukan kata-kata yang paling mungkin untuk dimanfaatkan untuk melanjutkan jawaban.\n",
    "\n",
    "### 3.2 Kapan Akan Berhenti Merangkai Jawaban\n",
    "\n",
    "Model LLM akan berhenti dalam memberikan jawaban ketika, model sudah mencapai batas token dalam merangkai sebuah kalimat. Sebagai contoh,\n",
    "\n",
    "- GPT:\n",
    "    - Model tertua GPT‑3 hanya 2.048 token\n",
    "    - GPT‑3.5‑Turbo umum: 4K, versi “16k” dukung hingga 16K\n",
    "    - GPT‑4 basic: 8K; varian 32k: 32K\n",
    "    - GPT‑4 Turbo: 128K input, output tetap 4K\n",
    "    - GPT‑4.1: hingga 1 juta token total\n",
    "\n",
    "- LLaMA (Meta)\n",
    "    - LLaMA 1 & 2: sekitar 2 048–4 096 token \n",
    "    - LLaMA 3 (8B, 70B, 405B): hingga 128 000 token \n",
    "    - LLaMA 3.2/3.3: tetap 128 000 token \n",
    "    - LLaMA 4 Scout: 10 juta token \n",
    "    - LLaMA 4 Maverick: 1 juta token\n",
    "\n",
    "- Gemini:\n",
    "    - Gemini 1 (Nano, dst.): 32 768 token \n",
    "    - Gemini 1.5 Flash/Pro:\n",
    "        - Flash standar: 128 000 token\n",
    "        - Pro (private/enterprise): hingga 1 000 000 token; kini sudah mulai buka akses hingga 2 000 000 token \n",
    "    - Gemini 2.5 Flash: 1 048 576 input, output hingga 65 536 token\n",
    "\n",
    "- DeepSeek\n",
    "    - DeepSeek‑LLM (7B & 67B): 4 096 token \n",
    "    - DeepSeek‑Coder: 16 384 token\n",
    "    - DeepSeek‑R1/V2/V3:\n",
    "        - R1/V2: 128 000 token \n",
    "        - V3: konteks 128 000 token\n",
    "\n",
    "Selain dari batasan token, model LLM akan selalu memprediksi sebuah probabilitas yang diberikan `End Of Sequence`/`<eos>`. Ketika hasil probabilitas untuk kata berikutnya adalah `<eos>`, maka model akan memberhentikan dalam membuat jawaban, walaupun tokennya masih tersedia banyak. \n",
    "\n",
    "Mengapa EOS Penting?\n",
    "- fisiensi: Model tidak terus memproses output yang tidak dibutuhkan.\n",
    "- ualitas teks: Mencegah model \"mengoceh\" atau menghasilkan teks yang tidak relevan.\n",
    "- ontrol output: Digunakan saat menyusun pipeline NLP seperti summarization, translation, dll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7904f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: sebuah tempat untuk kita nantinya memberikan sebuah inputan/pertanyaan\n",
    "# context: untuk meberikan data yang ingin coba dipelajari atau diproses \n",
    "# api_key: untuk kita memasukan key, untuk mengakses model generative ai secara online\n",
    "def generate_answer(query, context, api_key):\n",
    "    # untuk memasukan api key dari generative model yang digunakan\n",
    "    openai.api_key = api_key\n",
    "    # untuk memberitahu secara spesifik apa yang perlu dilakukan oleh model generative ai\n",
    "    system_message = \"Kamu adalah asisten cerdas yang menjawab pertanyaan berdasarkan data yang diberikan.\"\n",
    "    # untuk user memberikan inputan pertanyaan ataupun data yang ingin dipelajari\n",
    "    user_message = f\"\"\"\n",
    "    Pertanyaan: {query}\n",
    "\n",
    "    Data yang relevan:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = 'gpt-4.1-mini', # model yang digunakan\n",
    "        # sysmtem messages atau untuk mengolah inputan data ataupun user\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {'role': \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        # untuk mengatur tingkat pemilihan prediksi kata berikutnya\n",
    "        temperature = 0.3,\n",
    "        # untuk mengatur jumlah token yang bisa diproses\n",
    "        max_tokens = 1000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be771ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fee036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b78024cc",
   "metadata": {},
   "source": [
    "Temperature \n",
    "\n",
    "- 0 -> Model generative memilih probabilitas yang paling tinggi \n",
    "    + aku -> 88%\n",
    "    + saya -> 89% -> yang dipilih ini\n",
    "- 0.5 -> Model generative memilih probabilitas bukan yang paling tinggi, top 3 - top 5\n",
    "    + aku -> 88%\n",
    "    + saya -> 89%\n",
    "    + gue -> 74% -> yang dipilih ini\n",
    "    + abdi - 70%\n",
    "- 1.2 -> model generative memilih probabilitas semakin random\n",
    "    * Pilihan kata 1\n",
    "        + aku -> 88%\n",
    "        + saya -> 89% -> ini yang dipilih\n",
    "        + gue -> 74% \n",
    "        + abdi - 70%\n",
    "    * Pilihan kata 2\n",
    "        + adalah 99%\n",
    "        + apel 40% -> ini yang dipilih\n",
    "        + manusia 98%\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88efbf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def generate_answer(query, context, api_key):\n",
    "    openai.api_key = api_key\n",
    "    system_message = \"Kamu adalah asisten cerdas yang menjawab pertanyaan berdasarkan data yang diberikan.\"\n",
    "    user_message = f\"\"\"\n",
    "    Pertanyaan: {query}\n",
    "\n",
    "    Data yang relevan:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5589651a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gunung tertinggi di dunia adalah Gunung Everest, dengan ketinggian sekitar 8.848 meter (29.029 kaki) di atas permukaan laut. Gunung Everest terletak di Pegunungan Himalaya, di perbatasan antara Nepal dan Tibet.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "def generate_answer(query, context, api_key):\n",
    "    # Untuk versi OpenAI API terbaru (1.x.x ke atas), inisialisasi client seperti ini\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "    system_message = \"Kamu adalah asisten cerdas yang menjawab pertanyaan berdasarkan data yang diberikan.\"\n",
    "    user_message = f\"\"\"\n",
    "    Pertanyaan: {query}\n",
    "\n",
    "    Data yang relevan:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(  # Perubahan di sini\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    return response.choices[0].message.content # Perubahan di sini untuk mengakses konten\n",
    "\n",
    "# Contoh penggunaan:\n",
    "# Pastikan Anda mengganti 'YOUR_API_KEY' dengan kunci API OpenAI Anda yang sebenarnya.\n",
    "# Saya telah mengosongkan api_key di sini karena ini adalah contoh publik.\n",
    "# Anda perlu mengisi api_key Anda saat menjalankan kode ini.\n",
    "print(generate_answer(query= 'manakah gunung tertinggi di dunia?',\n",
    "                context= f\"\"\" There are at least 108 mountains on Earth with elevations of 7,200 m (23,622 ft; 4 mi) or greater above sea level. Of these, 14 are more than 8,000 m (26,247 ft; 5 mi).[1] The vast majority of these mountains are part of either the Himalayas or the Karakoram mountain ranges located on the edge of the Indian Plate and Eurasian Plate in China, India, Nepal, and Pakistan.\"\"\",\n",
    "                api_key= ''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
